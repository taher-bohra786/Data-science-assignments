{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.\n",
    "Gradient Boosting Regression is an ensemble learning method that builds multiple weak learners (usually decision trees) sequentially. Each new tree corrects the residual errors of the previous trees using gradient descent optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0003\n",
      "R-squared Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Sample dataset (X: input, y: target values)\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 3.6, 5.1, 7.3, 9.0])\n",
    "\n",
    "# Hyperparameters\n",
    "n_estimators = 50  # Number of trees\n",
    "learning_rate = 0.1\n",
    "max_depth = 2\n",
    "\n",
    "# Initialize predictions with the mean of y\n",
    "y_pred = np.full_like(y, np.mean(y), dtype=np.float64)\n",
    "\n",
    "# Train weak learners sequentially\n",
    "trees = []\n",
    "for _ in range(n_estimators):\n",
    "    residuals = y - y_pred  # Compute residual errors\n",
    "    tree = DecisionTreeRegressor(max_depth=max_depth)  # Weak learner\n",
    "    tree.fit(X, residuals)  # Fit to residuals\n",
    "    y_pred += learning_rate * tree.predict(X)  # Update predictions\n",
    "    trees.append(tree)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best R² Score: 0.8100951272649303\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Sample dataset \n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
    "y = np.array([1.5, 3.6, 5.1, 7.3, 9.0, 10.5, 13.1, 14.8, 17.0, 19.3])\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "cv_strategy = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform Grid Search with Cross-Validation\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=cv_strategy, scoring='r2', error_score='raise')\n",
    "grid_search.fit(X, y)  # Fit the model\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best R² Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.  \n",
    "A weak learner is a simple model (typically a shallow decision tree) that performs slightly better than random guessing. Gradient Boosting combines multiple weak learners to build a strong model.\n",
    "\n",
    "\n",
    "\n",
    "### **Q5.**  \n",
    "Gradient Boosting iteratively improves predictions by fitting new models to the residual errors of the previous models. It minimizes the loss function using gradient descent, refining predictions at each step.\n",
    "\n",
    "\n",
    "### **Q6.**  \n",
    "1. Start with an initial prediction (often the mean of the target values).  \n",
    "2. Calculate residuals (errors) between actual values and predictions.  \n",
    "3. Train a weak learner (decision tree) to predict the residuals.  \n",
    "4. Update the model’s predictions by adding the new learner’s output multiplied by a learning rate.  \n",
    "5. Repeat this process for multiple iterations, gradually reducing the errors.\n",
    "\n",
    "\n",
    "\n",
    "### **Q7.**  \n",
    "1. **Define the loss function** (e.g., Mean Squared Error for regression).  \n",
    "2. **Compute residuals** as negative gradients of the loss function.  \n",
    "3. **Fit a weak learner** to predict residuals.  \n",
    "4. **Update predictions** using the learning rate.  \n",
    "5. **Repeat** the process for multiple iterations.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
