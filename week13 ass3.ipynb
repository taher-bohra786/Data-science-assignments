{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Filter method is a feature selection technique that evaluates the relevance of each feature independently of the model. It relies on statistical techniques to rank features based on their relationship with the target variable. Common statistical measures used in the Filter method include:\n",
    "\n",
    "Correlation Coefficients (e.g., Pearson, Spearman)\n",
    "Chi-Square Test\n",
    "Mutual Information\n",
    "Variance Threshold\n",
    "It works by calculating a score for each feature and selecting the top-ranked features based on a predefined threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wrapper method differs from the Filter method as it evaluates feature subsets by training a machine learning model and measuring its performance. Instead of evaluating features individually, it considers the interaction between features.\n",
    "\n",
    "Common approaches in the Wrapper method include:\n",
    "\n",
    "Forward Selection (Start with no features and add the best one iteratively)\n",
    "\n",
    "Backward Elimination (Start with all features and remove the least important ones iteratively)\n",
    "\n",
    "Recursive Feature Elimination (RFE) (Eliminate features recursively using model importance scores)\n",
    "\n",
    "Unlike the Filter method, the Wrapper method is computationally expensive but often yields better results since it selects features based on model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded methods incorporate feature selection within the model training process. Common techniques include:\n",
    "\n",
    "Lasso Regression (L1 Regularization) – Shrinks less important feature coefficients to zero.\n",
    "\n",
    "Ridge Regression (L2 Regularization) – Assigns small weights to less important features.\n",
    "\n",
    "Decision Tree-Based Methods (e.g., Random Forest, XGBoost, LightGBM) – Use feature importance scores to rank \n",
    "features.\n",
    "\n",
    "Gradient Boosting Models (GBM, XGBoost, CatBoost, etc.) – Provide built-in feature selection using impurity-based ranking.\n",
    "\n",
    "These methods are efficient since they perform feature selection while training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignores feature interactions – It evaluates each feature independently, missing important feature dependencies.\n",
    "\n",
    "Not tailored to a specific model – The selected features may not be optimal for a particular machine learning model.\n",
    "\n",
    "May retain redundant features – Features that provide similar information may not be removed.\n",
    "\n",
    "Threshold Selection – The choice of threshold for feature selection is often arbitrary and may not always yield the best results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with high-dimensional datasets – Wrapper methods can be computationally expensive.\n",
    "\n",
    "Need for fast feature selection – Filter methods are faster since they do not require training models multiple \n",
    "times.\n",
    "\n",
    "Working with a general-purpose feature selection approach – Since the Filter method is independent of any specific model, it provides flexibility.\n",
    "\n",
    "Pre-processing step before Wrapper or Embedded methods – To reduce the number of features before applying a more computationally expensive method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n a telecom company working on customer churn prediction, the Filter method can be used as follows:\n",
    "\n",
    "Remove low-variance features – Features with almost the same values across all customers (e.g., an ID column) can be dropped.\n",
    "\n",
    "Calculate correlation – Compute the correlation between each feature and the target variable (churn or no churn). Remove weakly correlated features.\n",
    "\n",
    "Chi-Square Test – For categorical features, perform a chi-square test to check their relationship with the churn variable.\n",
    "\n",
    "Mutual Information – Use mutual information scores to select features that provide the most information about churn.\n",
    "\n",
    "Set a threshold – Select only the top features based on correlation/mutual information scores for further model training.\n",
    "\n",
    "This ensures that only the most relevant features are retained without computationally expensive model evaluations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For predicting the outcome of a soccer match using the Embedded method, follow these steps:\n",
    "\n",
    "Train a Decision Tree-Based Model (e.g., Random Forest, XGBoost). These models provide feature importance scores based on decision splits.\n",
    "\n",
    "L1 Regularization (Lasso Regression) – Train a linear model with L1 regularization to shrink coefficients of unimportant features to zero.\n",
    "\n",
    "Gradient Boosting Feature Importance – Use models like LightGBM or XGBoost to analyze feature importance scores.\n",
    "\n",
    "Drop Low-Importance Features – Remove features with near-zero importance to simplify the model.\n",
    "\n",
    "Train the Model Again – Retrain the model with the selected features and validate performance.\n",
    "\n",
    "This ensures that only the most predictive features (e.g., team rankings, recent form, key player statistics) are retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a limited number of features, the Wrapper method is suitable for selecting the best features for predicting house prices. The process includes:\n",
    "\n",
    "Use Recursive Feature Elimination (RFE) –\n",
    "\n",
    "Start with all features (size, location, age, etc.).\n",
    "\n",
    "Train a model (e.g., Linear Regression, Decision Tree).\n",
    "\n",
    "Remove the least important feature iteratively until the best subset is found.\n",
    "\n",
    "\n",
    "Forward Selection –\n",
    "\n",
    "Start with no features and add the most important one at each step.\n",
    "\n",
    "Continue until model performance stops improving.\n",
    "\n",
    "\n",
    "Backward Elimination –\n",
    "Start with all features and remove the least impactful ones.\n",
    "\n",
    "Cross-Validation –\n",
    "\n",
    "Use k-fold cross-validation to ensure robustness.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
