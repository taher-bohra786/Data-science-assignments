{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. \n",
    "**Random Forest Regressor** is an ensemble learning algorithm that combines multiple decision trees to predict continuous numerical values. It reduces variance and improves predictive accuracy by averaging the outputs of individual trees.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2.   \n",
    "Random Forest Regressor reduces overfitting by:  \n",
    "1. **Bootstrapping (Bagging)** – Training each decision tree on a different random subset of the data.  \n",
    "2. **Feature Randomness** – Each tree considers only a random subset of features, preventing reliance on specific attributes.  \n",
    "3. **Averaging Predictions** – The final prediction is the average of all tree outputs, reducing variance.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3.  \n",
    "Random Forest Regressor aggregates predictions by taking the **mean** (average) of the outputs of all individual decision trees in the ensemble. This averaging process smooths out individual tree errors, leading to a more robust and stable prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4.  \n",
    "Key hyperparameters include:  \n",
    "- **n_estimators** – Number of decision trees in the forest.  \n",
    "- **max_depth** – Maximum depth of each tree.  \n",
    "- **min_samples_split** – Minimum number of samples required to split a node.  \n",
    "- **min_samples_leaf** – Minimum number of samples in a leaf node.  \n",
    "- **max_features** – Maximum number of features considered for each split.  \n",
    "- **bootstrap** – Whether to use bootstrapped samples (default: True).  \n",
    "- **random_state** – Controls randomness for reproducibility.  \n",
    "\n",
    "---\n",
    "\n",
    "### Q5. \n",
    "| Feature | Decision Tree Regressor | Random Forest Regressor |\n",
    "|---------|-------------------------|--------------------------|\n",
    "| **Model Type** | Single tree | Ensemble of multiple trees |\n",
    "| **Overfitting** | Prone to overfitting | Less prone due to averaging |\n",
    "| **Stability** | Sensitive to data variations | More stable and generalizes better |\n",
    "| **Variance** | High | Lower due to averaging multiple predictions |\n",
    "| **Prediction** | Single tree output | Mean of all tree outputs |\n",
    "\n",
    "---\n",
    "\n",
    "### Q6.  \n",
    "**Advantages**  \n",
    "- Reduces overfitting by averaging multiple trees.  \n",
    "- Handles missing values and categorical data well.  \n",
    "- Works well with large datasets and high-dimensional data.  \n",
    "- Less sensitive to noisy data.  \n",
    "\n",
    "**Disadvantages**  \n",
    "- Slower training time compared to a single decision tree.  \n",
    "- Less interpretable than a single decision tree.  \n",
    "- Requires more memory and computational resources.\n",
    "\n",
    "---\n",
    "\n",
    "### Q7. \n",
    "The output is a **continuous numerical value**, calculated as the **average of predictions** from all decision trees in the ensemble.\n",
    "\n",
    "---\n",
    "\n",
    "### Q8.  \n",
    "No, **Random Forest Regressor** is specifically designed for regression tasks. However, **Random Forest Classifier** is a variant of the algorithm used for classification tasks, where it aggregates predictions using majority voting instead of averaging.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
