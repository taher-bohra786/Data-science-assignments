{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answers:**\n",
    "\n",
    "#### **Q1. What is Bayes' theorem?**  \n",
    "Bayes' theorem is a mathematical formula used to update the probability of a hypothesis based on new evidence. It describes how prior knowledge affects the likelihood of an event occurring.\n",
    "\n",
    "#### **Q2. What is the formula for Bayes' theorem?**  \n",
    "Bayes' theorem is given by:  \n",
    "\\[\n",
    "P(A | B) = \\frac{P(B | A) P(A)}{P(B)}\n",
    "\\]  \n",
    "where:  \n",
    "- \\( P(A | B) \\) is the posterior probability (probability of event \\( A \\) given \\( B \\) has occurred).  \n",
    "- \\( P(B | A) \\) is the likelihood (probability of event \\( B \\) given \\( A \\) is true).  \n",
    "- \\( P(A) \\) is the prior probability of \\( A \\).  \n",
    "- \\( P(B) \\) is the marginal probability of \\( B \\).\n",
    "\n",
    "#### **Q3. How is Bayes' theorem used in practice?**  \n",
    "Bayes' theorem is widely used in:  \n",
    "- **Spam Filtering**: Classifying emails as spam or not based on word probabilities.  \n",
    "- **Medical Diagnosis**: Estimating disease probability based on symptoms and prior probabilities.  \n",
    "- **Machine Learning (Naïve Bayes Classifier)**: Predicting categories of data.  \n",
    "- **Risk Analysis**: Assessing risks in finance, insurance, and fraud detection.  \n",
    "- **Natural Language Processing (NLP)**: Sentiment analysis, topic classification, etc.\n",
    "\n",
    "#### **Q4. What is the relationship between Bayes' theorem and conditional probability?**  \n",
    "Bayes' theorem is derived from the definition of conditional probability:  \n",
    "\\[\n",
    "P(A | B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "\\]  \n",
    "It provides a way to reverse conditional probabilities, i.e., calculating \\( P(A | B) \\) when \\( P(B | A) \\), \\( P(A) \\), and \\( P(B) \\) are known.\n",
    "\n",
    "#### **Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?**  \n",
    "The choice of Naïve Bayes classifier depends on the type of data:  \n",
    "- **Gaussian Naïve Bayes** (GNB): For continuous data that follows a normal distribution (e.g., sensor readings).  \n",
    "- **Multinomial Naïve Bayes** (MNB): For discrete, count-based data (e.g., text classification).  \n",
    "- **Bernoulli Naïve Bayes** (BNB): For binary feature data (e.g., word presence in documents).  \n",
    "- **Complement Naïve Bayes** (CNB): A variation of MNB, useful when dealing with imbalanced text classification problems.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. Naïve Bayes Classification for Given Data**\n",
    "\n",
    "#### **Step 1: Compute Likelihoods**  \n",
    "The likelihood is calculated as:  \n",
    "\\[\n",
    "P(X_i | C) = \\frac{\\text{Frequency of } X_i \\text{ in class } C}{\\text{Total count of class } C}\n",
    "\\]  \n",
    "\n",
    "From the table:  \n",
    "- **Total instances in class A:** \\( 3+3+4 = 10 \\)  \n",
    "- **Total instances in class B:** \\( 2+2+1 = 5 \\)  \n",
    "\n",
    "For \\( X_1 = 3 \\):  \n",
    "\\[\n",
    "P(X_1 = 3 | A) = \\frac{4}{10} = 0.4\n",
    "\\]\n",
    "\\[\n",
    "P(X_1 = 3 | B) = \\frac{1}{5} = 0.2\n",
    "\\]\n",
    "\n",
    "For \\( X_2 = 4 \\):  \n",
    "\\[\n",
    "P(X_2 = 4 | A) = \\frac{3}{10} = 0.3\n",
    "\\]\n",
    "\\[\n",
    "P(X_2 = 4 | B) = \\frac{3}{5} = 0.6\n",
    "\\]\n",
    "\n",
    "#### **Step 2: Compute Posterior Probabilities**  \n",
    "Assuming equal priors \\( P(A) = P(B) = 0.5 \\), we calculate:  \n",
    "\n",
    "\\[\n",
    "P(A | X_1 = 3, X_2 = 4) \\propto P(X_1 = 3 | A) P(X_2 = 4 | A) P(A)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= (0.4) \\times (0.3) \\times (0.5) = 0.06\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "P(B | X_1 = 3, X_2 = 4) \\propto P(X_1 = 3 | B) P(X_2 = 4 | B) P(B)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= (0.2) \\times (0.6) \\times (0.5) = 0.06\n",
    "\\]\n",
    "\n",
    "#### **Step 3: Compare and Predict**  \n",
    "Since \\( P(A | X_1 = 3, X_2 = 4) = P(B | X_1 = 3, X_2 = 4) \\), the classifier may choose either class. In real applications, a tie-breaking rule (like favoring the more frequent class) would be used. However, based purely on calculations, the probabilities are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
