{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "**Q1.**  \n",
    "- **Euclidean Distance**: Measures the straight-line distance between two points.  \n",
    "- **Manhattan Distance**: Measures the sum of absolute differences along each axis (grid-based).  \n",
    "**Impact on KNN**:  \n",
    "- Euclidean works well when data is continuous and evenly distributed.  \n",
    "- Manhattan is better for high-dimensional or grid-structured data (e.g., city block movements).  \n",
    "\n",
    "---  \n",
    "\n",
    "**Q2.**  \n",
    "- Use **cross-validation** to test multiple values.  \n",
    "- Small **K** → High variance (overfitting).  \n",
    "- Large **K** → High bias (underfitting).  \n",
    "- A common rule: **K ≈ sqrt(N) (N = number of training samples).**  \n",
    "- Use techniques like **Grid Search** or **Elbow Method** to find the best K.  \n",
    "\n",
    "---  \n",
    "\n",
    "**Q3.**  \n",
    "- **Euclidean Distance**: Works well when features have similar scales.  \n",
    "- **Manhattan Distance**: Preferred for high-dimensional, sparse, or structured data.  \n",
    "- **Minkowski Distance**: A generalization of both.  \n",
    "- **Cosine Similarity**: Useful when magnitude doesn’t matter, like text data.  \n",
    "**Choice depends on the data distribution and problem type.**  \n",
    "\n",
    "---  \n",
    "\n",
    "**Q4.**  \n",
    "1. **K (number of neighbors)** – Controls bias-variance tradeoff.  \n",
    "2. **Distance metric** – Euclidean, Manhattan, etc., impact similarity measurement.  \n",
    "3. **Weighting scheme** – Uniform vs. distance-weighted voting.  \n",
    "4. **Algorithm for neighbor search** – Brute-force vs. KD-Tree for efficiency.  \n",
    "**Tuning methods**: Grid Search, Cross-Validation, Hyperparameter optimization libraries.  \n",
    "\n",
    "---  \n",
    "\n",
    "**Q5.**  \n",
    "- **Large dataset** → Slow prediction time (distance calculations increase).  \n",
    "- **Small dataset** → Higher risk of overfitting.  \n",
    "**Optimization techniques**:  \n",
    "- Feature selection/dimensionality reduction (PCA).  \n",
    "- KD-Tree or Ball-Tree for faster neighbor searches.  \n",
    "- Use a **stratified sample** to balance training efficiency.  \n",
    "\n",
    "---  \n",
    "\n",
    "**Q6.**  \n",
    " **Slow for large datasets** → Use KD-Trees, Ball-Trees, Approximate Nearest Neighbors.  \n",
    " **Sensitive to irrelevant features** → Apply feature selection and scaling.  \n",
    " **Struggles with high dimensions** → Use PCA or dimensionality reduction.  \n",
    " **Sensitive to imbalanced data** → Use weighted KNN or resampling techniques.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
