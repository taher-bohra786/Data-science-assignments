{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q1. What is a projection and how is it used in PCA?**  \n",
    "A **projection** in PCA refers to mapping high-dimensional data points onto a lower-dimensional subspace while retaining as much variance as possible. Each data point is projected onto a new set of orthogonal axes (principal components) that maximize variance. This reduces dimensionality while preserving important patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. How does the optimization problem in PCA work, and what is it trying to achieve?**  \n",
    "PCA solves an **optimization problem** by finding a set of orthogonal axes (principal components) that maximize the variance of the projected data. It aims to:  \n",
    "1. Find directions (eigenvectors) that capture the most variance in the data.  \n",
    "2. Minimize reconstruction error when projecting data onto these principal components.  \n",
    "\n",
    "Mathematically, it maximizes:  \n",
    "\\[\n",
    "\\max_{w} \\quad w^T S w\n",
    "\\]\n",
    "where \\( S \\) is the covariance matrix of the data and \\( w \\) is the eigenvector. This is solved by computing eigenvalues and eigenvectors of \\( S \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. What is the relationship between covariance matrices and PCA?**  \n",
    "The **covariance matrix** in PCA captures how different features in the dataset vary together. It is given by:  \n",
    "\\[\n",
    "S = \\frac{1}{n} X^T X\n",
    "\\]\n",
    "where \\( X \\) is the mean-centered data matrix. The **eigenvectors** of this covariance matrix define the principal components, and the **eigenvalues** represent the variance explained by each component.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4. How does the choice of the number of principal components impact the performance of PCA?**  \n",
    "- **Too many components**: The model retains noise and redundancy, reducing interpretability.  \n",
    "- **Too few components**: Useful information may be lost, leading to poor reconstruction.  \n",
    "- **Optimal choice**: Usually determined by keeping enough components to explain **95-99%** of the variance (using the cumulative sum of eigenvalues).\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?**  \n",
    "PCA transforms features into **principal components** ranked by variance. By selecting the top components, it performs implicit feature selection.  \n",
    "**Benefits:**  \n",
    " Removes redundant features.  \n",
    " Reduces dimensionality and improves efficiency.  \n",
    " Helps mitigate multicollinearity.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. What are some common applications of PCA in data science and machine learning?**  \n",
    "- **Image compression**: Reduces storage while preserving image details.  \n",
    "- **Noise reduction**: Removes irrelevant variations in data.  \n",
    "- **Data visualization**: Converts high-dimensional data into 2D/3D for better understanding.  \n",
    "- **Feature extraction**: Improves model performance in classification/regression tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Q7. What is the relationship between spread and variance in PCA?**  \n",
    "- **Spread** refers to how dispersed the data is across different dimensions.  \n",
    "- **Variance** quantifies the spread numerically (i.e., larger variance means higher spread).  \n",
    "- PCA selects directions where **spread (variance) is maximized** to retain as much information as possible.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q8. How does PCA use the spread and variance of the data to identify principal components?**  \n",
    "1. Computes the covariance matrix to measure spread and relationships between features.  \n",
    "2. Finds **eigenvectors** (principal components) that align with directions of maximum variance.  \n",
    "3. Sorts components by **eigenvalues**, selecting those that retain most variance.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Q9. How does PCA handle data with high variance in some dimensions but low variance in others?**  \n",
    "- PCA **prioritizes** high-variance dimensions, making them dominant in projections.  \n",
    "- Low-variance dimensions contribute less and may be **discarded** if they don't explain much variance.  \n",
    "- This ensures that PCA focuses on the most **informative** features, reducing noise from unimportant dimensions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
