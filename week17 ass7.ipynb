{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. \n",
    "Boosting is an ensemble learning technique that combines multiple weak learners (typically decision trees) to create a strong learner by sequentially training models, where each new model corrects the errors of the previous ones.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. \n",
    "**Advantages:**  \n",
    "- Improves prediction accuracy.  \n",
    "- Reduces bias and variance.  \n",
    "- Works well with weak learners.  \n",
    "\n",
    "**Limitations:**  \n",
    "- Sensitive to noisy data and outliers.  \n",
    "- Computationally expensive.  \n",
    "- Can overfit if not properly tuned.  \n",
    "\n",
    "---\n",
    "\n",
    "### Q3.   \n",
    "Boosting works by training models sequentially. Each new model gives more weight to misclassified samples, improving their chances of correct classification in the next iteration. The final prediction is a weighted combination of all models.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4.   \n",
    "- **AdaBoost (Adaptive Boosting)**  \n",
    "- **Gradient Boosting (GBM)**  \n",
    "- **XGBoost (Extreme Gradient Boosting)**  \n",
    "- **LightGBM (Light Gradient Boosting Machine)**  \n",
    "- **CatBoost (Categorical Boosting)**  \n",
    "\n",
    "---\n",
    "\n",
    "### Q5.   \n",
    "- **n_estimators** – Number of weak learners.  \n",
    "- **learning_rate** – Controls contribution of each model.  \n",
    "- **max_depth** – Depth of weak learners (trees).  \n",
    "- **min_samples_split** – Minimum samples to split a node.  \n",
    "- **subsample** – Fraction of data used in each iteration.  \n",
    "\n",
    "---\n",
    "\n",
    "### Q6.  \n",
    "Boosting combines weak learners sequentially, assigning higher weights to misclassified samples. The final prediction is a weighted sum or majority vote of all weak learners.\n",
    "\n",
    "---\n",
    "\n",
    "### Q7.  \n",
    "AdaBoost (Adaptive Boosting) assigns weights to samples, increasing weights for misclassified ones. It trains weak learners sequentially, and the final prediction is a weighted sum of all learners.\n",
    "\n",
    "---\n",
    "\n",
    "### Q8.   \n",
    "AdaBoost minimizes the **exponential loss function**, which penalizes misclassified samples exponentially.\n",
    "\n",
    "---\n",
    "\n",
    "### Q9. \n",
    "AdaBoost increases the weights of misclassified samples so that the next weak learner focuses more on them. Correctly classified samples get lower weights.\n",
    "\n",
    "---\n",
    "\n",
    "### Q10.  \n",
    "Increasing the number of estimators improves performance up to a point but may lead to overfitting if too many are added.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
