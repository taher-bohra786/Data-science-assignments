{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?**  \n",
    "A **contingency matrix** is a table that compares the predicted labels of a classifier with the true labels. It helps evaluate classification performance by showing the number of correct and incorrect predictions for each class.  \n",
    "\n",
    "For a binary classification problem, a contingency matrix is also called a **confusion matrix**, which consists of:  \n",
    "- **True Positives (TP)** – Correctly predicted positive samples.  \n",
    "- **False Positives (FP)** – Incorrectly predicted positive samples.  \n",
    "- **False Negatives (FN)** – Incorrectly predicted negative samples.  \n",
    "- **True Negatives (TN)** – Correctly predicted negative samples.  \n",
    "\n",
    "For multi-class classification, the matrix extends to compare predictions across multiple classes. It is useful in computing precision, recall, F1-score, and other performance metrics.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations?**  \n",
    "A **pair confusion matrix** evaluates clustering results by considering how pairs of data points are assigned to clusters rather than comparing individual predictions to true labels.  \n",
    "\n",
    "Unlike a **regular confusion matrix**, which directly compares predicted labels to true labels, a **pair confusion matrix** assesses whether pairs of points are:  \n",
    "- In the **same** cluster in both true and predicted labels (True Positive Pair).  \n",
    "- In **different** clusters in both true and predicted labels (True Negative Pair).  \n",
    "- In the **same** cluster in true labels but **different** in predictions (False Negative Pair).  \n",
    "- In **different** clusters in true labels but **same** in predictions (False Positive Pair).  \n",
    "\n",
    "**Use Case:**  \n",
    "- Useful in clustering tasks where **label assignments are unknown** but pairwise relationships can be analyzed.  \n",
    "- Helps evaluate algorithms like **K-means** or **hierarchical clustering** without requiring explicit labels.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models?**  \n",
    "An **extrinsic measure** evaluates a model based on its performance in a real-world task rather than analyzing its internal components.  \n",
    "\n",
    "**Examples in NLP:**  \n",
    "- **Machine Translation:** BLEU score measures translation accuracy based on human translations.  \n",
    "- **Speech Recognition:** Word Error Rate (WER) assesses transcription accuracy.  \n",
    "- **Chatbots:** Response relevance and coherence scores from human evaluation.  \n",
    "\n",
    "**Why Use It?**  \n",
    "- Directly measures how well an NLP model contributes to end applications.  \n",
    "- Ensures that model improvements lead to better task performance.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?**  \n",
    "An **intrinsic measure** evaluates a model **internally** without relying on a downstream task.  \n",
    "\n",
    "**Examples:**  \n",
    "- **Perplexity** in language models (measures uncertainty in predicting the next word).  \n",
    "- **BLEU, ROUGE, METEOR** for text generation models.  \n",
    "- **Silhouette Score** in clustering.  \n",
    "\n",
    "**Difference from Extrinsic Measures:**  \n",
    "- **Intrinsic:** Evaluates the model's intermediate outputs (e.g., word embeddings, clusters).  \n",
    "- **Extrinsic:** Evaluates the model based on a real-world task.  \n",
    "\n",
    "**Example:**  \n",
    "- An **intrinsic** evaluation of a word embedding model would check cosine similarity between words.  \n",
    "- An **extrinsic** evaluation would test how well those embeddings improve sentiment analysis performance.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify strengths and weaknesses of a model?**  \n",
    "A **confusion matrix** is a table that compares actual vs. predicted classifications. It helps:  \n",
    "- **Detect class imbalances** by showing how often each class is misclassified.  \n",
    "- **Identify bias** if certain classes are consistently misclassified.  \n",
    "- **Compute key metrics** like precision, recall, and F1-score.  \n",
    "\n",
    "**Example Analysis:**  \n",
    "- If **FN is high**, the model has **poor recall** (many false negatives).  \n",
    "- If **FP is high**, the model has **poor precision** (many false positives).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms, and how can they be interpreted?**  \n",
    "Common **intrinsic measures** for clustering include:  \n",
    "\n",
    "1. **Silhouette Coefficient** (Measures compactness and separation)  \n",
    "   - **Range:** -1 to 1 (Higher is better).  \n",
    "\n",
    "2. **Davies-Bouldin Index** (Measures compactness and similarity between clusters)  \n",
    "   - **Lower values indicate better clustering.**  \n",
    "\n",
    "3. **Calinski-Harabasz Index** (Ratio of within-cluster to between-cluster dispersion)  \n",
    "   - **Higher values indicate better-defined clusters.**  \n",
    "\n",
    "4. **Dunn Index** (Ratio of smallest inter-cluster distance to largest intra-cluster distance)  \n",
    "   - **Higher values mean well-separated clusters.**  \n",
    "\n",
    "These measures assess **how well-defined and separated clusters are** without requiring true labels.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and how can these limitations be addressed?**  \n",
    "**Limitations of Accuracy:**  \n",
    "1. **Fails in imbalanced datasets**  \n",
    "   - If 95% of samples belong to one class, a model predicting everything as that class gets **95% accuracy** but is useless.  \n",
    "\n",
    "2. **Does not distinguish between precision and recall**  \n",
    "   - A model with high accuracy may still miss important **minority-class predictions** (low recall).  \n",
    "\n",
    "3. **Does not account for misclassification cost**  \n",
    "   - In medical diagnosis, a **false negative** (missing a disease) is worse than a **false positive** (misdiagnosing a healthy person).  \n",
    "\n",
    "**Better Alternatives:**  \n",
    "- **Precision & Recall** (when class distribution matters).  \n",
    "- **F1-score** (harmonic mean of precision and recall).  \n",
    "- **ROC-AUC Score** (for assessing how well the model distinguishes classes).  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
