{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** A decision tree classifier is a supervised learning algorithm that splits data based on feature conditions to form a tree-like structure. It makes predictions by traversing the tree from the root to a leaf node based on input feature values.  \n",
    "\n",
    "**Q2:** The algorithm uses metrics like Gini impurity or entropy to determine the best feature splits. It selects a feature that minimizes impurity, recursively splitting data until reaching pure or nearly pure leaf nodes.  \n",
    "\n",
    "**Q3:** For binary classification, the decision tree partitions the feature space into regions, assigning each region to one of two classes. At each node, it evaluates a feature condition and directs samples accordingly until a final classification is reached.  \n",
    "\n",
    "**Q4:** Geometrically, a decision tree divides the feature space into axis-aligned regions. Each split creates a hyperplane parallel to a feature axis, segmenting the space into distinct classification regions.  \n",
    "\n",
    "**Q5:** A confusion matrix is a table that summarizes a classification model’s performance by comparing predicted vs. actual labels. It contains True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).  \n",
    "\n",
    "**Q6:** Example confusion matrix:  \n",
    "\\[\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " & Predicted Positive & Predicted Negative \\\\\n",
    "\\hline\n",
    "Actual Positive & TP = 40 & FN = 10 \\\\\n",
    "\\hline\n",
    "Actual Negative & FP = 5 & TN = 45 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\]  \n",
    "Precision = \\( TP/TP + FP \\) = \\(40/40+5 \\) = 0.89  \n",
    "Recall = \\( TP/TP + Fp \\) = \\(40/40+10 \\) = 0.80  \n",
    "F1 Score = \\( 2 * (Precision*Recall)/(Precision + Recall) \\) = 0.84  \n",
    "\n",
    "**Q7:** The choice of evaluation metric depends on the problem's objective. Accuracy is good for balanced data, precision for minimizing false positives, recall for minimizing false negatives, and F1-score for balanced importance of both.  \n",
    "\n",
    "**Q8:** Spam detection—precision is crucial because false positives (legitimate emails marked as spam) should be minimized to avoid losing important emails.  \n",
    "\n",
    "**Q9:** Disease diagnosis—recall is crucial because false negatives (missed diagnoses) should be minimized to ensure all patients needing treatment are identified.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
